model:
  _target_: transformers.AutoModelForCausalLM.from_pretrained
  pretrained_model_name_or_path: null  # must be set
  trust_remote_code: true
  device_map: "auto"
tokenizer:
  _target_: transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: null  # must be set
  trust_remote_code: false 
  use_fast: true
openapi: false
max_seq_length: 2048
dataset_dir: null  # must be set
strict: false
target_dataset: "all-with-nc"
log_dir: null  # must be set
torch_dtype: "bf16"
custom_prompt_template: null
custom_fewshots_template: null
resource_dir: null

prompt_dump_dir: null  # must be set
offline_dir: null  # must be set

wandb:
  launch: false
  log: false
  entity: null
  project: null
  run_name: null

metainfo:
  version: "1.4.1"
  basemodel_name: "None"
  model_type: "llm-jp"
  instruction_tuning_method_by_llm_jp: "None"
  instruction_tuning_data_by_llm_jp: ["None"]
  data_type: "dev"
  num_few_shots: 4
  max_num_samples: 100

pipeline_kwargs:
  add_special_tokens: False
  prefix: ""

generator_kwargs:
  do_sample: False
  top_p: 1.0
  #top_k: 0
  #temperature: 0.1
  repetition_penalty: 1.0

hydra:
  job:
    env_set:
      TOKENIZERS_PARALLELISM: false
